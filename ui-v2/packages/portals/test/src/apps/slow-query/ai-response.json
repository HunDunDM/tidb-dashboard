{
  "markdown-content": "### **Key Observations**\n1. **High Execution Time**:\n   - The query took **11 minutes and 9 seconds** to execute, which is extremely slow for a query with a `LIMIT 101`.\n\n2. **Index Usage**:\n   - The query uses the index `index_orders_on_created_at` to filter rows based on the `created_at` range (`2024-04-07 18:07:52` to `2024-05-11 18:07:52`).\n   - The `IndexRangeScan_35` operator scanned **25,147,450 rows**, but only **12,082,311 rows** matched the additional filter on `orders.id`.\n\n3. **Table Lookups**:\n   - The `TableRowIDScan_36` operator performed a table lookup to fetch the remaining columns for the filtered rows. This step took **54 minutes and 46.2 seconds**, which is the majority of the query execution time.\n\n4. **Filter Conditions**:\n   - The query has multiple filter conditions:\n     - `orders.mode = 'production'`\n     - `orders.user_id = 11111`\n     - `orders.label_id IS NOT NULL`\n     - `orders.created_at` range\n     - `orders.id` range (`1000000000` to `1500000000`)\n\n5. **Ordering and Limit**:\n   - The `TopN_10` operator applies the `ORDER BY orders.id DESC` and `LIMIT 101` clauses, which is efficient because it only processes the first 101 rows after filtering.\n\n---\n\n### **Root Cause Analysis**\nThe root cause of the slow query performance is:\n- **Suboptimal Index Usage**: The query uses the `index_orders_on_created_at` index, which is not selective enough for the combination of filters (`created_at`, `id`, `mode`, `user_id`, and `label_id`).\n- **Large Data Scans**: The `IndexRangeScan_35` operator scanned **25 million rows**, and the `TableRowIDScan_36` operator processed **12 million rows**, which is excessive for a query with a `LIMIT 101`.\n- **Table Lookup Overhead**: The `TableRowIDScan_36` operator added significant overhead because it had to fetch the remaining columns from the table for each filtered row.\n\n---\n\n### **Proposed Solutions for SQL Tuning**\n\n#### **1. Create a Composite Index**\nThe current indexes are not optimal for this query. Create a **composite index** that includes all the columns used in the `WHERE` clause and the `ORDER BY` clause. For example:\n```sql\nCREATE INDEX orders_covering_idx ON orders (user_id, created_at, id, mode, label_id);\n```\n\nThis index will:\n- Allow the query to quickly locate rows based on `user_id`, `created_at`, and `id`.\n- Eliminate the need for a table lookup because all required columns are included in the index.\n\n#### **2. Rewrite the Query (if necessary)**\nThe query is already well-written, but if additional filters or joins are added in the future, ensure that the new index is used. For example:\n```sql\nSELECT `orders`.*\nFROM `orders`\nWHERE \n    `orders`.`mode` = 'production'\n    AND `orders`.`user_id` = 11111\n    AND orders.label_id IS NOT NULL\n    AND orders.created_at >= '2024-04-07 18:07:52'\n    AND orders.created_at <= '2024-05-11 18:07:52'\n    AND orders.id >= 1000000000\n    AND orders.id < 1500000000\nORDER BY orders.id DESC \nLIMIT 101;\n```\n\n#### **3. Verify the New Execution Plan**\nAfter creating the new index, use `EXPLAIN ANALYZE` to verify that the query uses the new index and that the performance has improved. The new execution plan should look something like this:\n```sql\n+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------+----------+------+\n| id                             | estRows   | actRows | task      | access object                                                                  | execution info                                      | operator info                                                                          | memory   | disk |\n+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------+----------+------+\n| TopN_10                        | 101.00    | 101     | root      |                                                                                | time:100ms, loops:2                                 | orders.id:desc, offset:0, count:101                                                    | 271 KB   | N/A  |\n| └─IndexRangeScan_35            | 101.00    | 101     | cop[tikv] | table:orders, index:orders_covering_idx(user_id, created_at, id, mode, label_id)| tikv_task:{proc max:2ms, min:0s, avg: 1ms, p...}   | range:[11111 2024-04-07 18:07:52 1000000000,11111 2024-05-11 18:07:52 1500000000), ... | N/A      | N/A  |\n+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------+----------+------+\n```\n\n#### **4. Update Statistics**\nEnsure that the statistics for the `orders` table are up-to-date so the optimizer can make better decisions. Run:\n```sql\nANALYZE TABLE orders;\n```\n\n---\n\n### **Expected Outcome**\nWith the new composite index (`orders_covering_idx`):\n- The query should no longer need to perform table lookups, as all required columns are included in the index.\n- The `IndexRangeScan` operator should process fewer rows, and the `TopN` operator should complete much faster.\n- The overall query execution time should be significantly reduced, potentially from **11 minutes and 9 seconds** to **100 milliseconds** or less.\n\n---\n\n### **Additional Considerations**\n1. **Index Size**:\n   - The new index (`orders_covering_idx`) will be larger than the current indexes because it includes multiple columns. However, the performance benefits for this query will likely outweigh the additional storage overhead.\n\n2. **Write Performance**:\n   - If the `orders` table is frequently updated (e.g., inserts, updates, or deletes), the new index may slightly impact write performance. Monitor the system to ensure that the trade-off is acceptable.\n\n3. **Query Patterns**:\n   - If this query is part of a larger workload, consider whether other queries could also benefit from the new index. If so, the index may provide additional performance improvements across the workload.\n\n---\n\n### **Conclusion**\nBy creating a composite index that includes all the columns used in the `WHERE` clause and the `ORDER BY` clause, you can eliminate the need for table lookups and significantly improve the performance of this query. The new index should reduce the query execution time and make the query more efficient, especially if it is executed frequently or as part of a larger workload."
}
